{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc07308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c55b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBelief(Optimizer):\n",
    "    def __init__(self, params, lr = 0.1, betas = (0.9,0.9), eps = 1e-6, weight_decay = 0, correct_bias = True):\n",
    "        \n",
    "        if lr<0:\n",
    "            raise ValueError(\"Invalid learning rate\")\n",
    "        if not 0<= betas[0]<=1:\n",
    "            raise ValueEttor(\"Invalid beta 0\")\n",
    "        if not 0<= betas[1]<=1:\n",
    "            raise ValueEttor(\"Invalid beta 1\")\n",
    "        if not eps<=0:\n",
    "            raise ValueEttor(\"Invalid epsilon\")\n",
    "        defaults = dict(lr = lr, betas= betas, eps = eps, weight_decay = weight_decay, correct_bias = correct_bias)\n",
    "        \n",
    "        super(AdaBelief, self).__init__(params, defaults)\n",
    "        \n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        \n",
    "        \n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                \n",
    "                \n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                \n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Grads are sparse')\n",
    "                \n",
    "                \n",
    "                state = self.state[param]\n",
    "                \n",
    "                if len(state)==0:\n",
    "                    \n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(param.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(param.data)\n",
    "                    \n",
    "                \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta_1, beta_2 = group['betas']\n",
    "                state['step'] +=1\n",
    "                \n",
    "                \n",
    "                \n",
    "                m_t = torch.add(torch.mul(beta_1, exp_avg),torch.mul(1.0-beta,grad))\n",
    "                s_t = torch.add(torch.add(torch.mul(beta_2, exp_avg_sq),torch.mul(1.0-beta_2,torch.square(torch.sub(grad,m_t)))),group['eps'])\n",
    "                \n",
    "                state['exp_avg'] = m_t\n",
    "                state['exp_avg_sq'] = s_t\n",
    "                \n",
    "                if group['correct_bias']:\n",
    "                    m_t = m_t.divide(1.0-beta_1**state['step'])\n",
    "                    s_t = s_t.divide(1.0-beta_2**state['step'])\n",
    "                \n",
    "                denom = torch.add(torch.sqrt(s_t),group['eps'])\n",
    "                param.data = param.data.addcdiv(m_t,denom,value = -state['lr'])\n",
    "                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
